{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1\n",
      "0.2.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "import torchvision\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          ...,\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       " \n",
       "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          ...,\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       " \n",
       "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          ...,\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]),\n",
       " tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.8952, 0.4661, 0.1074, 0.0789, 1.0000, 0.8952, 0.4661, 0.1074,\n",
       "           0.0789, 1.0000, 1.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000]]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DeepLesionDataset(Dataset):\n",
    "    \"\"\"DeepLesion dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_table(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                os.path.join(\n",
    "                                    *self.data_frame.iloc[idx, 0].rsplit('_', 1)\n",
    "                                ))\n",
    "\n",
    "        image = io.imread(img_name).astype(np.int32)\n",
    "#         image = np.expand_dims(image, axis=2)\n",
    "        image = image - 32768\n",
    "        image = image.astype(np.int32)\n",
    "        \n",
    "\n",
    "        \n",
    "        img_min = image.min()\n",
    "        img_max = image.max()\n",
    "        \n",
    "        low_bound = -500\n",
    "        high_bound = 500\n",
    "        \n",
    "        \n",
    "        channel_1 = (((image-img_min)/(low_bound-img_min))*255).clip(0, 255)\n",
    "        channel_2 = (((image-low_bound)/(high_bound-low_bound))*255).clip(0, 255)\n",
    "        channel_3 = (((image-high_bound)/(img_max-high_bound))*255).clip(0, 255)\n",
    "\n",
    "        image = np.zeros((3, image.shape[0], image.shape[1]))\n",
    "        image[0] = channel_1\n",
    "        image[1] = channel_2\n",
    "        image[2] = channel_3\n",
    "        image = image.astype(np.uint8).transpose(1,2,0)\n",
    "\n",
    "        bounding_box = np.array(self.data_frame.iloc[idx].Bounding_boxes.split(','))\n",
    "        bounding_box = bounding_box.astype('float').reshape(-1, 2)\n",
    "        label = self.data_frame.iloc[idx].Coarse_lesion_type.astype(int) - 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         bounding_box = [c_x1, c_y1, c_x2, c_y2]\n",
    "\n",
    "        \n",
    "      \n",
    "        c_x =  ((bounding_box[1][0] + bounding_box[0][0])/2) / image.shape[0]\n",
    "        c_y =  ((bounding_box[1][1] + bounding_box[0][1])/2) / image.shape[1]\n",
    "        c_hw = (bounding_box[1][0] - bounding_box[0][0]) / image.shape[0]\n",
    "        c_hh = (bounding_box[1][1] - bounding_box[0][1]) / image.shape[1]\n",
    "#         c_x1 = bounding_box[0][0] / image.shape[0]\n",
    "#         c_y1 = bounding_box[0][1] / image.shape[1]\n",
    "#         c_x2 = bounding_box[1][0] / image.shape[0]\n",
    "#         c_y2 = bounding_box[1][1] / image.shape[1]\n",
    "\n",
    "       \n",
    "        grid_dim = 7\n",
    "        target = torch.zeros((grid_dim,grid_dim,11))\n",
    "        \n",
    "        cell_dim = 1 / grid_dim\n",
    "\n",
    "        target[int(c_y // cell_dim), int(c_x // cell_dim),0] = c_x\n",
    "        target[int(c_y // cell_dim), int(c_x // cell_dim),1] = c_y\n",
    "        target[int(c_y // cell_dim), int(c_x // cell_dim),2] = c_hw\n",
    "        target[int(c_y // cell_dim), int(c_x // cell_dim),3] = c_hh\n",
    "        target[int(c_y // cell_dim), int(c_x // cell_dim),4] = 1 # b1_c\n",
    "        \n",
    "        target[int(c_y // cell_dim), int(c_x // cell_dim),5] = c_x\n",
    "        target[int(c_y // cell_dim), int(c_x // cell_dim),6] = c_y\n",
    "        target[int(c_y // cell_dim), int(c_x // cell_dim),7] = c_hw\n",
    "        target[int(c_y // cell_dim), int(c_x // cell_dim),8] = c_hh\n",
    "        target[int(c_y // cell_dim), int(c_x // cell_dim),9] = 1 # b2_c\n",
    "        target[int(c_y // cell_dim), int(c_x // cell_dim),10] = 1 # c_c\n",
    "                \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "    \n",
    "    \n",
    "        return image, target\n",
    "\n",
    "d = DeepLesionDataset('../sets/train_set_small.tsv', '/media/mark/Data/deeplesion/Images_png',\n",
    "                     transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(448),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                     ])\n",
    "                     )\n",
    "\n",
    "d[193]\n",
    "# for i in d:\n",
    "#     pass\n",
    "# print(d[0][0].numpy().transpose(1, 2, 0).shape)\n",
    "# for row in d[0][0].squeeze().numpy():\n",
    "#     print(row)\n",
    "# plt.close('all')\n",
    "# fig,ax = plt.subplots(1, figsize=(5,5))\n",
    "# fig = plt.figure(figsize=(5, 5))\n",
    "# _ = plt.imshow(d[0][0].squeeze().numpy())\n",
    "# plt.show()\n",
    "# bb_x = d['bounding_box'][0,0]\n",
    "# bb_y = d['bounding_box'][0,1]\n",
    "# bb_w = d['bounding_box'][1,0] - d['bounding_box'][0,0] \n",
    "# bb_h = d['bounding_box'][0,1] - d['bounding_box'][1,1]\n",
    "# rect1 = patches.Rectangle((bb_x,bb_y),bb_w,bb_h,linewidth=1,edgecolor='r',facecolor='none')\n",
    "# ax.add_patch(rect1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1., -1.], [1., -1.]]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, bb, pred, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "#     print(inp.numpy().shape)\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "#     print(inp.shape)\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    _ = plt.imshow(inp)\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    ax.add_patch(patches.Rectangle(\n",
    "        (\n",
    "            (bb[0] - bb[2]/2) * inp.shape[0],\n",
    "            (bb[1] - bb[3]/2) * inp.shape[1]\n",
    "        ),\n",
    "        (bb[2]) * inp.shape[0],\n",
    "        (bb[3]) * inp.shape[1],\n",
    "        linewidth=1,edgecolor='b',facecolor='none'))\n",
    "    ax.add_patch(patches.Rectangle(\n",
    "        (\n",
    "            (pred[0] - pred[2]/2) * inp.shape[0],\n",
    "            (pred[1] - pred[3]/2) * inp.shape[1]\n",
    "        ),\n",
    "        (pred[2]) * inp.shape[0],\n",
    "        (pred[3]) * inp.shape[1],\n",
    "        linewidth=1,edgecolor='w',facecolor='none'))\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "#     plt.show()\n",
    "\n",
    "# inputs, classes, bb = next(iter(dataloaders['train']))   \n",
    "\n",
    "# out = torchvision.utils.make_grid(inputs)\n",
    "# inputs\n",
    "# imshow(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6, typ='val'):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "#     fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels, bb) in enumerate(dataloaders[typ]):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            bb = bb.to(device)\n",
    "\n",
    "            outputs = torch.sigmoid(model(inputs))\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "#                 ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "#                 ax.axis('off')\n",
    "#                 ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j], bb.cpu().data[j], outputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(448),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        \n",
    "                     ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(448),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        \n",
    "                     ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(448),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "                     ]),\n",
    "}\n",
    "# DeepLesionDataset('./test_set.tsv', '/media/mark/Data/deeplesion/Images_png')\n",
    "\n",
    "file_names = {'train': '../sets/validation_set_small.tsv',#train_set_small.tsv', \n",
    "              'val': '../sets/validation_set_small.tsv', \n",
    "              'test': '../sets/test_set_small.tsv'}\n",
    "\n",
    "image_datasets = {x: DeepLesionDataset(file_names[x], '/media/mark/Data/deeplesion/Images_png',\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val', 'test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=1,\n",
    "                                             shuffle=True, num_workers=1)\n",
    "              for x in ['train', 'val', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "# class_names = image_datasets['train']['labels']\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "#     best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#     best_acc = 0.0\n",
    "    best_loss = 1000000.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, targets in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                     # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(targets)\n",
    "#                     print(model.features[0].weight)\n",
    "#                     outputs = torch.sigmoid(model(inputs))\n",
    "#                     _, preds = torch.max(outputs, 1)\n",
    "                    \n",
    "                    loss = criterion(outputs, targets)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item()# / inputs.size(0)\n",
    "                # running_loss += loss.item() loss.item() * inputs.size(0)\n",
    "#                 running_corrects += torch.sum(preds == bb.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "#             epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "#             print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "#                 phase, \n",
    "#                 epoch_loss, \n",
    "#                 epoch_acc))\n",
    "            print('{} Loss: {:.4f}'.format(\n",
    "                phase, \n",
    "                epoch_loss, \n",
    "#                 epoch_acc\n",
    "            ))\n",
    "\n",
    "    \n",
    "#             # deep copy the model\n",
    "#             if phase == 'val' and epoch_acc > best_acc:\n",
    "#                 best_acc = epoch_acc\n",
    "#                 best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "#                 best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "#     visualize_model(model, 16, typ='train')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLONet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        # I would like to keep image in houndfield units but it might be tricky\n",
    "        # Dont know if I should use their windows or not - probably should start with this\n",
    "        \"\"\"\n",
    "        YOLO layout\n",
    "        inp : 448 x 448 x 3 \n",
    "\n",
    "        1: (conv (7 x 7) x 64 -> mp (2 x 2) x s2 ) x 2\n",
    "        out: 112 x 112 x 64\n",
    "        2: (conv (3 x 3) x 192) -> mp (2 x 2) x s2) x 1\n",
    "        out: 56 x 56 x 192\n",
    "        3: conv:\n",
    "            (1 x 1) x 128\n",
    "            (3 x 3) x 256\n",
    "            (1 x 1) x 256\n",
    "            (3 x 3) x 512\n",
    "           mp: (2 x 2) x s2\n",
    "        out: 28 x 28 x 512\n",
    "        4: conv:\n",
    "            (1 x 1) x 256 -| x 4\n",
    "            (3 x 3) x 512 -|\n",
    "            (1 x 1) x 512\n",
    "            (3 x 3) x 1024\n",
    "           mp: (2 x 2) x s2\n",
    "        out: 14 x 14 x 1024  ### possibly keep at this resolution for higher res grid\n",
    "        5: conv:\n",
    "            (1 x 1) x 512  -| x 2\n",
    "            (3 x 3) x 1024 -|\n",
    "            (3 x 3) x 1024\n",
    "            (3 x 3) x 1024 s2\n",
    "        out: 7 x 7 x 1024\n",
    "        6: conv:\n",
    "            ((3 x 3) x 1024) x 2\n",
    "        out: 7 x 7 x 1024\n",
    "        7: FC -> 4096\n",
    "        8: FC -> 7x7x10\n",
    "        \"\"\"\n",
    "        super(YOLONet, self).__init__()\n",
    "        # in : 448 x 448 x 1\n",
    "        # out: 112 x 112 x 64\n",
    "        self.features = torch.nn.Sequential( \n",
    "            nn.Conv2d(3, 64, 7, padding=3),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(64, 64, 7, padding=3),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "        \n",
    "        # in : 112 x 112 x 64\n",
    "        # out: 56 x 56 x 192\n",
    "        \n",
    "            nn.Conv2d(64, 192, 3, padding=1),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "        \n",
    "        # in : 56 x 56 x 192\n",
    "        # out: 28 x 28 x 512\n",
    "            nn.Conv2d(192, 128, 1),\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.Conv2d(256, 256, 1),\n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "        # in : 28 x 28 x 512\n",
    "        # out: 14 x 14 x 1024\n",
    "            nn.Conv2d(512, 256, 1),\n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.Conv2d(512, 256, 1),\n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.Conv2d(512, 256, 1),\n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.Conv2d(512, 256, 1),\n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.Conv2d(512, 512, 1),\n",
    "            nn.Conv2d(512, 1024, 3, padding=1),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "        # in : 14 x 14 x 1024\n",
    "        # out: 7 x 7 x 1024\n",
    "            nn.Conv2d(1024, 512, 1),\n",
    "            nn.Conv2d(512, 1024, 3, padding=1),\n",
    "            nn.Conv2d(1024, 512, 1),\n",
    "            nn.Conv2d(512, 1024, 3, padding=1),\n",
    "            nn.Conv2d(1024, 1024, 3, padding=1),\n",
    "            nn.Conv2d(1024, 1024, 3, padding=1, stride=2),\n",
    "        # in : 7 x 7 x 1024\n",
    "        # out: 7 x 7 x 1024\n",
    "            nn.Conv2d(1024, 1024, 3, padding=1),\n",
    "            nn.Conv2d(1024, 1024, 3, padding=1),\n",
    "        )\n",
    "\n",
    "        \n",
    "        fc1 = nn.Linear(7 * 7 * 1024, 4096)\n",
    "        fc2 = nn.Linear(4096, 7 * 7 * 11)\n",
    "        \n",
    "        self.connected = nn.Sequential(\n",
    "            fc1, \n",
    "            fc2\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary (differentiable) operations on Tensors.\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.connected(x)\n",
    "        x = x.view(x.size(0), 7, 7, 11)\n",
    "        return torch.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# source : https://github.com/kevin970401/pytorch-YOLO-v1/blob/master/loss/loss.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "class yoloLoss(nn.Module):\n",
    "    def __init__(self, S, B, C, lambda_coord=5, lambda_noobj=0.5):\n",
    "        super(yoloLoss, self).__init__()\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "        \n",
    "    def calc_iou(self, A, B):\n",
    "        \"\"\"calc iou A & B\n",
    "        \n",
    "        Args:\n",
    "            A (torch.FloatTensor): [N, SxSx(Bx5+C)]\n",
    "            B (torch.FloatTensor): [N, SxSx(Bx5+C)]\n",
    "        \"\"\"\n",
    "\n",
    "        A = A.view(-1, self.S, self.S, self.B * 5 + self.C)\n",
    "        B = B.view(-1, self.S, self.S, self.B * 5 + self.C)\n",
    "        \n",
    "        A_x_center = A[:, :, :, 0:self.B*5:5]\n",
    "        A_y_center = A[:, :, :, 1:self.B*5:5]\n",
    "        A_w = A[:, :, :, 2:self.B*5:5]\n",
    "        A_h = A[:, :, :, 3:self.B*5:5]\n",
    "        \n",
    "        B_x_center = B[:, :, :, 0:self.B*5:5]\n",
    "        B_y_center = B[:, :, :, 1:self.B*5:5]\n",
    "        B_w = B[:, :, :, 2:self.B*5:5]\n",
    "        B_h = B[:, :, :, 3:self.B*5:5]\n",
    "        \n",
    "        A_area = A_w * A_h\n",
    "        B_area = B_w * B_h\n",
    "\n",
    "        inter_box_x0, _ = torch.max(torch.cat([(A_x_center-A_w/2).unsqueeze(dim=-1), (B_x_center-B_w/2).unsqueeze(dim=-1)], dim=-1), dim=-1)\n",
    "        inter_box_y0, _ = torch.max(torch.cat([(A_y_center-A_h/2).unsqueeze(dim=-1), (B_y_center-B_h/2).unsqueeze(dim=-1)], dim=-1), dim=-1)\n",
    "\n",
    "        inter_box_x1, _ = torch.min(torch.cat([(A_x_center+A_w/2).unsqueeze(dim=-1), (B_x_center+B_w/2).unsqueeze(dim=-1)], dim=-1), dim=-1)\n",
    "        inter_box_y1, _ = torch.min(torch.cat([(A_y_center+A_h/2).unsqueeze(dim=-1), (B_y_center+B_h/2).unsqueeze(dim=-1)], dim=-1), dim=-1)\n",
    "        \n",
    "        inter_box_w = inter_box_x1-inter_box_x0\n",
    "        inter_box_h = inter_box_y1-inter_box_y0\n",
    "\n",
    "        inter = inter_box_w * inter_box_h * (inter_box_h>0).float() * (inter_box_w>0).float()\n",
    "\n",
    "        iou = inter / (A_area + B_area - inter + 1e-6)\n",
    "\n",
    "        return iou\n",
    "        \n",
    "    def get_argmax_iou(self, A, B):\n",
    "        \"\"\"get argmax of iou A & B\n",
    "        \n",
    "        Args:\n",
    "            A (torch.FloatTensor): [N, SxSx(Bx5+C)]\n",
    "            B (torch.FloatTensor): [N, SxSx(Bx5+C)]\n",
    "        \"\"\"\n",
    "\n",
    "        iou = self.calc_iou(A, B)\n",
    "        \"\"\"iou: [N, S, S, B]\n",
    "        \"\"\"\n",
    "        \n",
    "        argmax = torch.argmax(iou, dim=-1)\n",
    "        return argmax\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"calc loss\n",
    "        \n",
    "        Args:\n",
    "            pred (torch.floatTensor): [N, S, S, (Bx5+C)]\n",
    "            target (torch.floatTensor): [N, S, S, Bx5+C] score is always equal to 1. bbox: [x_center, y_center, w, h]\n",
    "        \"\"\"\n",
    "        num_elements = self.B * 5 + self.C\n",
    "        num_batch = target.size(0)\n",
    "        \n",
    "        target = target.view(-1, self.S*self.S, num_elements)\n",
    "        pred = pred.view(-1, self.S*self.S, num_elements)\n",
    "        \"\"\"now target and pred: [N, SxS, (Bx5+C)]\n",
    "        \"\"\"\n",
    "\n",
    "        obj_mask = target[:,:,4] > 0\n",
    "        noobj_mask = target[:,:,4] == 0\n",
    "\n",
    "        obj_mask = obj_mask.unsqueeze(-1).expand_as(target).float()\n",
    "        noobj_mask = noobj_mask.unsqueeze(-1).expand_as(target).float()\n",
    "        \"\"\"now obj_mask and noobj: [N, SxS, (Bx5+C)]\n",
    "        \"\"\"\n",
    "        \n",
    "        responsible_bbox_arg = self.get_argmax_iou(pred, target)\n",
    "        responsible_bbox_scatter = torch.tensor((0, 1, 2, 3, 4))\\\n",
    "                                .repeat((num_batch, self.S * self.S, 1)).cuda()\\\n",
    "                                + responsible_bbox_arg.view(-1, self.S*self.S, 1)\n",
    "        responsible_bbox_mask = torch.zeros((num_batch, self.S * self.S, self.B * 5 + self.C)).cuda()\\\n",
    "                                .scatter_(2, responsible_bbox_scatter, torch.ones((num_batch, self.S * self.S, self.B * 5 + self.C)).cuda())\n",
    "        responsible_bbox_mask = responsible_bbox_mask * obj_mask\n",
    "\n",
    "        # class prediction loss\n",
    "        class_prediction_loss = ((torch.sigmoid(pred) - torch.sigmoid(target)) * obj_mask)[:, :, self.B*5:].pow(2).sum()\n",
    "\n",
    "        # no obj loss\n",
    "        noobj_loss = self.lambda_noobj * ((torch.sigmoid(pred) - torch.sigmoid(target)) * noobj_mask)[:, :, 4:self.B*5:5].pow(2).sum()\n",
    "\n",
    "        # obj loss\n",
    "        obj_loss = ((torch.sigmoid(pred) - torch.sigmoid(target)) * responsible_bbox_mask)[:, :, 4:self.B*5:5].pow(2).sum()\n",
    "\n",
    "        # coord loss\n",
    "        coord_xy_loss = self.lambda_coord * ((pred-target) * responsible_bbox_mask)[:, :, 0:self.B*5:5].pow(2).sum()\\\n",
    "                        + self.lambda_coord * ((pred-target) * responsible_bbox_mask)[:, :, 1:self.B*5:5].pow(2).sum()\n",
    "\n",
    "        coord_wh_loss = self.lambda_coord * ((pred-target) * responsible_bbox_mask)[:, :, 2:self.B*5:5].pow(2).sum()\\\n",
    "                        + self.lambda_coord * ((pred-target) * responsible_bbox_mask)[:, :, 3:self.B*5:5].pow(2).sum()\n",
    "        \n",
    "        total_loss = class_prediction_loss + noobj_loss + obj_loss + coord_xy_loss + coord_wh_loss\n",
    "\n",
    "        return total_loss/num_batch\n",
    "\n",
    "# model_ft = YOLONet()\n",
    "# model_ft = model_ft.to(device)\n",
    "    \n",
    "\n",
    "# loss = Loss(7, 2, 1)\n",
    "\n",
    "# for inputs, target in dataloaders['train']:\n",
    "#     inputs = inputs.to(device)\n",
    "#     target = labels.to(device)\n",
    "#     pred = model_ft(inputs)\n",
    "#     print(pred.shape, target.shape)\n",
    "#     print(loss(pred, target))\n",
    "#     break\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "# results = None\n",
    "# target = None\n",
    "# c = 0\n",
    "# model.train()\n",
    "# for e in range(20):\n",
    "#     for inputs, labels in dataloaders['train']:\n",
    "#         inputs = inputs.to(device)\n",
    "#         target = labels.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         results = model(inputs)\n",
    "#         loss = yoloLoss(7,2, 5, 1)(results, target)\n",
    "#         print(loss.item())\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         c += 1\n",
    "# #     break\n",
    "       \n",
    "# # print(target.shape)\n",
    "# # print(results.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_ft = YOLONet()\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# criterion = nn.SmoothL1Loss(reduction='sum')#reduction='sum')\n",
    "criterion = yoloLoss(7, 2, 1)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "# optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 24 epochs\n",
    "# exp_lr_scheduler = None#lr_scheduler.StepLR(optimizer_ft, step_size=100, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 7, 7], expected input[1, 7, 7, 11] to have 3 channels, but got 7 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8fe19037f6e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model_ft = train_model(model_ft, criterion, optimizer_ft, None,\n\u001b[0;32m----> 2\u001b[0;31m                        num_epochs=1)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-b4a594dc6e4e>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                      \u001b[0;31m# zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;31m#                     print(model.features[0].weight)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#                     outputs = torch.sigmoid(model(inputs))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/2018_health/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-a8f0d19af773>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mwell\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0marbitrary\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdifferentiable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0moperations\u001b[0m \u001b[0mon\u001b[0m \u001b[0mTensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \"\"\"\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/2018_health/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/2018_health/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/2018_health/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/2018_health/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 7, 7], expected input[1, 7, 7, 11] to have 3 channels, but got 7 channels instead"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, None,\n",
    "                       num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_ft.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_model(model_ft, 16, typ='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_output(model, criterion):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels, bb) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            bb = bb.to(device)\n",
    "\n",
    "            outputs = torch.sigmoid(model(inputs))\n",
    "\n",
    "            loss = criterion(outputs.view(-1, 1), bb.view(-1, 1))\n",
    "            print(outputs)\n",
    "            print(bb)\n",
    "            print('l', loss)\n",
    "            print(loss.sum(dim=1))\n",
    "#             print(loss.item())\n",
    "            return outputs, bb\n",
    "            break\n",
    "    model.train(mode=was_training)\n",
    "\n",
    "o, bb = check_output(model_ft, nn.PairwiseDistance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in model_ft.parameters():\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = YOLONet()\n",
    "pp = 0\n",
    "for p in model_ft.parameters():\n",
    "    if p.requires_grad:\n",
    "        n_n=1\n",
    "        for s in list(p.size()):\n",
    "            n_n = n_n*s\n",
    "        pp += n_n\n",
    "print(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): Dropout(p=0.5)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.alexnet(pretrained=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
